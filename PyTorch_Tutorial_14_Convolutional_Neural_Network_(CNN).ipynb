{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Tutorial 14 - Convolutional Neural Network (CNN).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcEH6h1b/MRw19OelFBPeI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/sample/blob/master/PyTorch_Tutorial_14_Convolutional_Neural_Network_(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://www.youtube.com/watch?v=pDdP0TFzsoQ&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=15)"
      ],
      "metadata": {
        "id": "WdA37ZcofBeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FZCNIjhre_fF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper parameters\n",
        "num_epoch = 64\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n"
      ],
      "metadata": {
        "id": "EcZI2VSPgVJv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting norm and std"
      ],
      "metadata": {
        "id": "qBr7-L03oIy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = torchvision.datasets.CIFAR10('/content',True,transforms.ToTensor(),download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQC2tP3whsCs",
        "outputId": "c54c3dde-55d4-4873-e2e8-b7992fd415b9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sample,_ = train_data[4]"
      ],
      "metadata": {
        "id": "1WKvObLmoRXX"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = one_sample / 2 + 0.5\n",
        "img = torchvision.utils.make_grid(img).numpy()\n",
        "img = np.transpose(img,(1,2,0)) # we changed shape from(3,32,32) into (32,32,3) means 0 dimension to the end and ..\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "k5P5j1lUxr5w",
        "outputId": "b67068e9-d5c1-48ae-e078-0d61424f5548"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f28bdad8250>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sample.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSt0muA-jBDe",
        "outputId": "19af6a79-ca12-42f3-b7d2-2e4d7afb3a33"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sample.mean([1,2])    # dimension means we calculate based on dimension of zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ICKJLkFj_sz",
        "outputId": "b4d4f75f-c936-41ea-a67e-0c92cb9a736d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5537, 0.4122, 0.2511])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_sample.std([1,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbDYwVaglOjJ",
        "outputId": "4962f7d4-dc09-4124-c0fa-894ade2d1a7d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1595, 0.1665, 0.1603])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = torch.zeros(1,3)\n",
        "std = torch.zeros(1,3)\n",
        "for i,(samples,lables) in enumerate(train_loader):\n",
        "  mean += samples.mean([2,3])\n",
        "  std += samples.std([2,3])\n",
        "\n",
        "mean = mean/(i+1)\n",
        "std = std/(i+1)\n",
        "print(mean)\n",
        "print(std)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgUoocjRjfrF",
        "outputId": "f5d1a85e-5bc5-4cd6-a483-e8740a75901d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4914, 0.4822, 0.4465]])\n",
            "tensor([[0.2023, 0.1994, 0.2010]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can use mean and std of the ImageNet dataset.\n",
        "\n",
        "mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "\n",
        "but our dataset's mean and std is a bit diffrent\n"
      ],
      "metadata": {
        "id": "7ywB-LdfqEGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))])"
      ],
      "metadata": {
        "id": "pk-xIfzjgoJ5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = torchvision.datasets.CIFAR10('/content',train=True,transform=transform,download=True)\n",
        "test_data = torchvision.datasets.CIFAR10('/content',train=False,transform=transform,download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8S_OaAdqYUK",
        "outputId": "d15f5b2e-a4e1-4188-c8c9-20aa91a9f122"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,...)\n",
        "    self.conv1 = nn.Conv2d(3,6,5) # input:3*32*32\n",
        "    # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0,...)\n",
        "    self.pool1 = nn.MaxPool2d(2,2) # input:6*28*28\n",
        "    self.conv2 = nn.Conv2d(6,16,5) # input:6*14*14   \n",
        "    self.pool2 = nn.MaxPool2d(2,2) # input:16*10*10   # we didnt need to define to maxpool layer because they have same properties\n",
        "    # torch.nn.Linear(in_features, out_features, bias=True,...)\n",
        "    self.fc1 = nn.Linear(16*5*5,120) #input: 16*5*5\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    out= F.relu(self.conv1(x))\n",
        "    out = self.pool1(out)\n",
        "    out= F.relu(self.conv2(out))\n",
        "    out = self.pool2(out)\n",
        "    out = out.view(-1,16*5*5) # ATTENTION: transition from conv to fc layer\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "MlAqxkhprkvu"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "7WEgCnRr5-Oq"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epoch):\n",
        "  for i,(images,labels) in enumerate(train_loader):\n",
        "\n",
        "    output = model(images)\n",
        "    loss = criterion(output,labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if(i+1) % 1000 == 0 :\n",
        "      print(f'epoch = {epoch}/{num_epoch}, steps {i+1}/{n_total_steps} , loss = {loss.item() :.4f}  ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRH95KxP7AbJ",
        "outputId": "48abb84e-5bd4-42eb-a171-1efb5abc169a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0/4, steps 1000/12500 , loss = 1.6746  \n",
            "epoch = 0/4, steps 2000/12500 , loss = 1.5005  \n",
            "epoch = 0/4, steps 3000/12500 , loss = 1.7504  \n",
            "epoch = 0/4, steps 4000/12500 , loss = 1.5491  \n",
            "epoch = 0/4, steps 5000/12500 , loss = 1.2851  \n",
            "epoch = 0/4, steps 6000/12500 , loss = 2.8026  \n",
            "epoch = 0/4, steps 7000/12500 , loss = 1.4126  \n",
            "epoch = 0/4, steps 8000/12500 , loss = 0.6606  \n",
            "epoch = 0/4, steps 9000/12500 , loss = 0.9203  \n",
            "epoch = 0/4, steps 10000/12500 , loss = 1.7541  \n",
            "epoch = 0/4, steps 11000/12500 , loss = 0.7473  \n",
            "epoch = 0/4, steps 12000/12500 , loss = 1.1094  \n",
            "epoch = 1/4, steps 1000/12500 , loss = 1.8322  \n",
            "epoch = 1/4, steps 2000/12500 , loss = 0.5767  \n",
            "epoch = 1/4, steps 3000/12500 , loss = 1.1036  \n",
            "epoch = 1/4, steps 4000/12500 , loss = 0.5726  \n",
            "epoch = 1/4, steps 5000/12500 , loss = 0.6688  \n",
            "epoch = 1/4, steps 6000/12500 , loss = 1.6008  \n",
            "epoch = 1/4, steps 7000/12500 , loss = 0.9979  \n",
            "epoch = 1/4, steps 8000/12500 , loss = 1.5634  \n",
            "epoch = 1/4, steps 9000/12500 , loss = 0.7195  \n",
            "epoch = 1/4, steps 10000/12500 , loss = 1.6545  \n",
            "epoch = 1/4, steps 11000/12500 , loss = 1.0095  \n",
            "epoch = 1/4, steps 12000/12500 , loss = 0.3493  \n",
            "epoch = 2/4, steps 1000/12500 , loss = 2.2043  \n",
            "epoch = 2/4, steps 2000/12500 , loss = 1.6887  \n",
            "epoch = 2/4, steps 3000/12500 , loss = 1.4240  \n",
            "epoch = 2/4, steps 4000/12500 , loss = 1.0825  \n",
            "epoch = 2/4, steps 5000/12500 , loss = 1.1582  \n",
            "epoch = 2/4, steps 6000/12500 , loss = 1.2776  \n",
            "epoch = 2/4, steps 7000/12500 , loss = 1.8594  \n",
            "epoch = 2/4, steps 8000/12500 , loss = 1.5370  \n",
            "epoch = 2/4, steps 9000/12500 , loss = 0.3758  \n",
            "epoch = 2/4, steps 10000/12500 , loss = 0.9858  \n",
            "epoch = 2/4, steps 11000/12500 , loss = 0.9895  \n",
            "epoch = 2/4, steps 12000/12500 , loss = 1.5293  \n",
            "epoch = 3/4, steps 1000/12500 , loss = 0.5505  \n",
            "epoch = 3/4, steps 2000/12500 , loss = 0.7931  \n",
            "epoch = 3/4, steps 3000/12500 , loss = 0.7824  \n",
            "epoch = 3/4, steps 4000/12500 , loss = 2.0298  \n",
            "epoch = 3/4, steps 5000/12500 , loss = 1.4504  \n",
            "epoch = 3/4, steps 6000/12500 , loss = 0.8265  \n",
            "epoch = 3/4, steps 7000/12500 , loss = 1.4657  \n",
            "epoch = 3/4, steps 8000/12500 , loss = 1.7129  \n",
            "epoch = 3/4, steps 9000/12500 , loss = 0.3785  \n",
            "epoch = 3/4, steps 10000/12500 , loss = 0.3277  \n",
            "epoch = 3/4, steps 11000/12500 , loss = 1.1328  \n",
            "epoch = 3/4, steps 12000/12500 , loss = 1.5479  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "Xltb1K1c-l2Q"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}