{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7IYbSCvDCIoAo0ADuQFit",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/sample/blob/master/Emojify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rufFNufqDmUO"
      },
      "source": [
        "# Emotion detection from text using PyTorch and Federated Learning\n",
        "\n",
        "For this project, we are going to implement an NLP task of creating a model to detect the emotion from text. We will develop this using the PyTorch library and the Federated Learning framework for decentralized training. \n",
        "\n",
        "We will create an emotion detection for the following 5 emotions:\n",
        "\n",
        "| Emotion | Emoji   | Label   |\n",
        "|------|------|------|\n",
        "|Loving| ‚ù§Ô∏è| 0|\n",
        "|Playful| ‚öΩÔ∏è| 1|\n",
        "|Happy| üòÑ| 2|\n",
        "|Annoyed| üòû| 3|\n",
        "|Foodie| üçΩ| 4|\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will work with a dataset (X, Y) where we have:\n",
        "*   X contains 132 sentences\n",
        "*   Y contains a label between [0, 4] corresponding to the five emotions.\n",
        "\n",
        "For example:\n",
        "\n",
        "| Sentence | Emotion   |\n",
        "|----------|-----------|\n",
        "|food is life|  üçΩ Foodie|\n",
        "|I love you mum|  ‚ù§Ô∏è Loving|\n",
        "|Stop saying bullshit|  üòû Annoyed|\n",
        "|congratulations on your acceptance|  üòÑ Happy|\n",
        "|The assignment is too long|    üòû Annoyed|\n",
        "|I want to go play| ‚öΩÔ∏è Playful|\n",
        "|she did not answer my text| üòû Annoyed|\n",
        "|Your stupidity has no limit| üòû Annoyed|\n",
        "|how many points did he score|  ‚öΩÔ∏è Playful|\n",
        "|my algorithm performs poorly| üòû Annoyed|\n",
        "|I got approved|  üòÑ Happy|\n",
        "\n",
        "## The Model\n",
        "We will build an LSTM model that takes as input word sequences that will take word ordering into account. We will use 50-dimensional [GloVe](https://nlp.stanford.edu/projects/glove/) pre-trained word embeddings to represent words. We will then feed those as an input into an LSTM that will predict the most appropiate emotion for the text. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1s-KYhU5JWF-jvAlZ2MIKKugxLLDdhpQP)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/mostafa-ja/coursera-deep-learning-specialization/master/C5%20-%20Sequence%20Models/Week%202/Emojify/data/train_emoji.csv'\n",
        "!wget 'https://raw.githubusercontent.com/mostafa-ja/coursera-deep-learning-specialization/master/C5%20-%20Sequence%20Models/Week%202/Emojify/data/test_emoji.csv'\n",
        "!wget 'https://ia803006.us.archive.org/1/items/glove.6B.50d-300d/glove.6B.50d.txt'\n"
      ],
      "metadata": {
        "id": "sLzJ34ArHM1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQn1wO2qr01C"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d04n5XXyf5Cj"
      },
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "\n",
        "def read_csv(filename):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = read_csv('/content/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('/content/test_emoji.csv')"
      ],
      "metadata": {
        "id": "XAdaQ1i1IxXx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtWINKCgI8ud",
        "outputId": "6958c4c5-2b16-4254-ca00-912a542cef19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(132,) (132,)\n",
            "(56,) (56,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0],Y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czbmBGHOJKyM",
        "outputId": "86b0d0f2-ab22-41a0-f930-d5ea944db55f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "never talk to me again 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.eye(5)[0])\n",
        "print(np.eye(5)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp7J62vdJ3jD",
        "outputId": "4e5960f1-cda7-42e1-b07a-8f873a3bef6f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_oh_train = np.eye(5)[Y_train]\n",
        "Y_oh_test = np.eye(5)[Y_test]\n",
        "print(Y_oh_train.shape)\n",
        "print(Y_oh_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjQfqekeJanm",
        "outputId": "6ae271e5-cc76-4906-fe1c-b6c65ae8600a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(132, 5)\n",
            "(56, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('/content/glove.6B.50d.txt', 'r')\n",
        "next(iter(data))\n",
        "#every line in data starts with a word then word's embedding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "jPAVHhyhL7wE",
        "outputId": "11d7c83b-3a51-42eb-ca7e-f5866f7bf9b5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/glove.6B.50d.txt', 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line1 = line.strip().split()\n",
        "            curr_word = line1[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line1[1:], dtype=np.float64)\n",
        "            break\n"
      ],
      "metadata": {
        "id": "rd2KlHNxLZh-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk4PNzqQLpWe",
        "outputId": "7293b9d2-7b98-49a5-ae30-0b79516b7107"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line1[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngR-I3laLr3u",
        "outputId": "586ffd4a-7366-4505-b251-51ded0eda2ca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', '0.418', '0.24968', '-0.41242', '0.1217']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_to_index: dictionary mapping from words to their indices in the vocabulary\n",
        "(400,001 words, with the valid indices ranging from 0 to 400,000)\n",
        "\n",
        "index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary\n",
        "\n",
        "word_to_vec_map: dictionary mapping words to their GloVe vector representation."
      ],
      "metadata": {
        "id": "TpSb_YYFOLeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/glove.6B.50d.txt')"
      ],
      "metadata": {
        "id": "g_rSgh1iKU83"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9jJeNhtoBnp"
      },
      "source": [
        "### 2.2 Pytorch and mini-batching \n",
        "\n",
        "* In this exercise, we want to train Keras using mini-batches. \n",
        "* However, most deep learning frameworks require that all sequences in the same mini-batch have the **same length**. \n",
        "    * This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it's just not possible to do them both at the same time.\n",
        "    \n",
        "#### Padding handles sequences of varying length\n",
        "* The common solution to handling sequences of **different length** is to use padding.  Specifically:\n",
        "    * Set a maximum sequence length\n",
        "    * Pad all sequences to have the same length. \n",
        "    \n",
        "##### Example of padding\n",
        "* Given a maximum sequence length of 20, we could pad every sentence with \"0\"s so that each input sentence is of length 20. \n",
        "* Thus, the sentence \"I love you\" would be represented as $(e_{I}, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\ldots, \\vec{0})$. \n",
        "* In this example, any sentences longer than 20 words would have to be truncated. \n",
        "* One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "id": "Rmz4rE8LoBnr"
      },
      "outputs": [],
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]\n",
        "    X_indices = np.zeros((m,max_len))\n",
        "\n",
        "    for i in range(m):\n",
        "      sentence = X[i]\n",
        "      words = sentence.lower().split()\n",
        "      for j,word in enumerate(words):\n",
        "        index = word_to_index[word]\n",
        "        X_indices[i][j] = index\n",
        "\n",
        "    return X_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLyIs_aFjdfl",
        "outputId": "9ec44ba3-4d96-443b-b3e8-5587b58aa03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X1 = np.array([\"lol\", \"I love you\", \"this is very yummy\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\", X1_indices)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 = ['lol' 'I love you' 'this is very yummy']\n",
            "X1_indices = [[225122.      0.      0.      0.      0.]\n",
            " [185457. 226278. 394475.      0.      0.]\n",
            " [358160. 192973. 377946. 394957.      0.]]\n"
          ]
        }
      ]
    }
  ]
}