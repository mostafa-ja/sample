{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPMMP99sNrh1KigC78ZtIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/sample/blob/master/Pytorch_Torchtext_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basics\n",
        "Torchtext is a very powerful library that solves the preprocessing of text very well, but we need to know what it can and can’t do, and understand how each API is mapped to our inherent understanding of what should be done. An additional perk is that Torchtext is designed in a way that it does not just work with PyTorch, but with any deep learning library (for example: Tensorflow).\n",
        "\n",
        "Let’s compile a list of tasks that text preprocessing must be able to handle. All checked boxes are functionalities provided by Torchtext.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **Train/Val/Test Split:** seperate your data into a fixed train/val/test set (not used for k-fold validation)\n",
        "*   **File Loading:** load in the corpus from various formats\n",
        "\n",
        "*   **Tokenization:** break sentences into list of words\n",
        "\n",
        "*   **Vocab:** generate a vocabulary list\n",
        "*   **Numericalize/Indexify:** Map words into integer numbers for the entire corpus\n",
        "\n",
        "*   **Word Vector:** either initialize vocabulary randomly or load in from a pretrained embedding, this embedding must be “trimmed”, meaning we only store words in our vocabulary into memory.\n",
        "\n",
        "\n",
        "*   **Batching:** generate batches of training sample (padding is normally happening here)\n",
        "*   **Embedding Lookup:** map each sentence (which contains word indices) to fixed dimension word vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "h0EGQVHkPjgp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8um2mH6PjQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa58efC6O5l1"
      },
      "outputs": [],
      "source": []
    }
  ]
}